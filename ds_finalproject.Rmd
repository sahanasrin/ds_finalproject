---
title: "Final Project"
author: "Sahana Srinivasan, Jake Kochmansky, Jae Gnazzo, Nathaniel Spilka"
output: html_document
---
# _________________________________________________

# PPOL 670 | Final Project

#Load Libraries
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#code for clearing up workspace
rm(list = ls(all.names = TRUE))
graphics.off()
cat("\014")

#loading in all relevant packages 
library(tidycensus)
library(tidyverse)
library(tidymodels)
library(parsnip)
library(recipes)
library(ranger)
library(yardstick)
library(vip)
library(stringr)
library(sf)
library(janitor)
library(censusapi)
library(jsonlite)
library(httr)
library(usmap)
library(here)
library(mapview)
library(ggthemes)
library(tigris)
#library(albersusa)
library(ggiraph)
library(srvyr)
#library(Hmisc)
```

##The question that lies at the heart of our analysis is: Using a small number of continuous variables, how accurately can our specified model predict median household income in United States Census Tracts? We believe this question to be critical in helping define salient predictors of the amount of wealth households in America are capable of accumulating. As of February 2022 per the Center of Poverty and Social Policy in Columbia University, poverty rates in the US were around 14.4%, with the expiration of Child Tax Credit (CTC) payments hastening the increase in childhood poverty rates. Thus, we believe that our question poses important implications to both short-term concerns related to the quality of life of American families as well as long-term indicators of economic growth and progress in the country at large. And, perhaps if certain unapparent variables demonstrate to have some level of predictive power over the outcome of median household income, digestable changes can be made in the lives of everyday Americans with serve to improve their lives.

##To put this question to test, we adopt a combination of supervised and unsupervised machine learning methods, as well as various Exploratory Data Analysis, API Queries, and Geospatial tools to guide our data collection and decisionmaking processes. The datasets we invoke for our analysis are the American Community Survey collected from the years of 2013-2017 reflecting data on all Census Tracts in all 50 states, as well as the Childhood Opportunity Index Dataset, which gives insight on the neigborhood conditions (ie., access to recereational services andclean natural resources) of children in all census tracts. 


#Census API Query Method 1 and Column Name Clean-Up

#We start the exploration of our question by gathering ACS data on various variables of interest. Among these are the aggregate time spent traveling to work by individuals in a given tract, the total count of females, the number of computers, the number of individuals who have completed high school, the number of people with a GED, the number of people who have completed an associate's degree, and the number of people who have completed a bachelor's degree. 
```{r - census API tracts}
#Loading in API, gathering data --------------------
#Census api key
#Sys.getenv("CENSUS_API_KEY")

#Loading in state variable list
states = c("CA", "OR", "WA", "NV", "AZ", "CO", "ID", "MT", "WY", "NM", "AK", "HI","UT",
           "ND", "SD", "NE", "KS", "MN", "IA", "MO", "WI", "IL", "IN", "MI", "OH",
           "OK", "TX", "LA", "AR", "MS", "AL", "GA", "FL", "TN", "KY", "WV", "VA", "NC", "SC", "DE","MD",
           "ME", "NH", "VT", "MA", "CT", "RI", "NJ", "PA", "NY")

#Browse variable names 
variables_2017 <- load_variables(2017, "acs5", cache = TRUE)

#Use get_acs method to retrieve desired data on all 50 states 
states_2017 <- get_acs(
  geography = "tract",
  variables = c(total_pop = "B01003_001", #total population
                travel_min_work_agg = "B08133_001", #aggregate travel time to work in minutes
                female_count = "B01001_026", #total female
                computer_count ="B28010_001", #computers in household
                education_alloc = "B99151_001", #allocation of educational attainment (number of people who have completed HS)
                med_income = "B19013_001", #median income level in last 12 months (this is our outcome variable)
                hs_count = "B15003_017", #number of people with a HS diploma
                ged_count = "B15003_018", #number of people with a ged
                assoc_count = "B15003_021", #number of people with an associates degree
                bach_count= "B15003_022" #number of people with bachelor's degree
                ), 
  output = "wide",
  state = states,
  year = 2017) # data from years 2013 - 2017
```


#Read in Childhood Opportunity Index Dataset and Join Census Data with New Dataset
#Now, we read in the Childhood Opportunity Index Dataset. The variables of interest here are Z-scores of overall Childhood Opportunity Indices nationally-normed, Health and Environmental Indices nationally-normed, and education domain nationally normed. We want to join this with our previous datasets using GEOID as the common link in order to conduct a more robust analysis. 
```{r - childhood opportunity}
#reading in childhood opportunity data 
child_opp_data1 <- read_csv("data/index.csv")

#clean childhood opportunity data 
child_opp_data2 <- child_opp_data1 %>%
  na.omit(child_opp_data1) %>%
  filter(year == 2015) %>%
  select(geoid, pop, msaname15, z_ED_nat, z_HE_nat, z_COI_nat) %>%
  clean_names() %>%
  rename(GEOID = geoid)

#joining the census and COI data
states_2017 <- left_join(states_2017, child_opp_data2, by= "GEOID")
```
Learn more about the dataset [here](https://www.diversitydatakids.org/child-opportunity-index)

##Cleaning the states_2017 joined dataset
```{r - cleaning data}
#this function will help us get the state from each census tract 
stateName  <- function(input) {
  tractCountyState <- unlist(strsplit(input, ", "))
  stateName <- tractCountyState[3]
  
  return(stateName)
}

#we're iterating through the states_2017 "names" column and using the function
#above to pull the state names
stateList <- map(.x = states_2017$NAME, .f = ~stateName(.x))
#we're converting the format to a tibble so we can add it to states_2017
stateTibble <- as.tibble(unlist(stateList))

#adding a state column to our census states_2017 datababy
states_2017 <- bind_cols(
  states_2017,
  stateTibble
)

#Renaming geoid and name columns 
states_2017 <- states_2017 %>%
  rename(geoid = GEOID,
         name = NAME,
         state_name = value)

#drop the E in the estimate columns
for ( col in 1:ncol(states_2017)){
    colnames(states_2017)[col] <-  sub("E.*", "", colnames(states_2017)[col])
}

#dropping error estimate columns
states_2017 <-states_2017 %>%
  select(-c("total_popM", "travel_min_work_aggM", "female_countM", "computer_countM", "computer_countM", "med_incomeM", "hs_countM",
            "ged_countM", "assoc_countM", "bach_countM"))

#removing all NAs
states_2017 <- na.omit(states_2017)

#turning all 0's to 1's for the log
states_2017[states_2017 == 0] <- 1

#creating the relative frequencies for all variables 
states_2017 <- states_2017 %>%
    mutate(
      travel_min_work_agg_prop = travel_min_work_agg/total_pop,
      female_count_prop = female_count/total_pop,
      computer_count_prop = computer_count/total_pop,
      hs_count_prop = hs_count/total_pop,
      ged_count_prop = ged_count/total_pop,
      assoc_count_prop = assoc_count/total_pop,
      bach_count_prop = bach_count/total_pop,
      education_alloc_prop = education_alloc/total_pop)

#it looks like we should center some variables, but not log everything
#we need to log GED, and med_income
#hist.data.frame(only_numb)

# #making all variables log
# states_2017 <- states_2017 %>%
#   mutate(
#     log_total_pop = log(total_pop),
#     log_travel_min_work_agg = log(travel_min_work_agg),
#     log_female_count = log(female_count),
#     log_computer_count = log(computer_count),
#     log_education_alloc = log(education_alloc),
#     log_med_income = log(med_income),
#     log_hs_count = log(hs_count),
#     log_ged_count = log(ged_count),
#     log_assoc_count = log(assoc_count),
#     log_bach_count = log(bach_count)
#     )
# #only using the numbers for the recipe
# only_numb <- states_2017 %>%
#   select(starts_with("log_"), starts_with("z"))
#removing inf
# only_numb <- only_numb %>%
#   filter_all(all_vars(!is.infinite(.)))

```

#Exploratory Data Analysis Phase 1: Understanding How Key Variables of Interest Interplay
#The goal of this initial EDA is to understand the distribution of American households across income brackets specified by the [FILL IN THE BLANK], as well as gain insight into features that characterize these income brackets. 

#The first vizualization titled "Number of People in the United States in Specified Income Brackets" shows us that the lower-median income bracket is the largest, with the median-upper income bracket following behind. In terms of a specific number of census tracts in each bracket, 

#The final vizualization titled "" demonstrates that average aggregated travel time for members in tracts, taken as a proportion of the total individuals in the given tract, is highest for those in the median-upper income level. 
```{r EDA}
#finding number of people in different income brackets
states_2017_EDA <- states_2017 %>% 
  mutate(
    income_bracket = case_when(
    med_income <= 25161 ~ "Lower", 
    med_income <= 78624 ~ "Lower-Median", 
    med_income <= 186151 ~ "Median-Upper", 
    TRUE ~ "Upper") 
  )

states_2017_EDA %>%
  ggplot(mapping = aes(x = income_bracket, y = total_pop)) + 
  geom_col() +
  labs (
    title = "Number of People in the United States in Specified Income Brackets", 
    subtitle = "Employment Figures From April 2021", 
    x = "Income Bracket", 
    y = "Number of People", 
    caption = ""
  )

#finding proportion of census tracts in specified income bracket 
# states_2017_EDA %>% 
#   mutate(lower_income = if_else(income_bracket == "Lower", 1, 0)) %>% #gen dummy variable 
#   mutate(lower_median_income =if_else(income_bracket == "Lower-Median", 1, 0)) %>%
#   mutate(median_upper_income = if_else(income_bracket == "Median-Upper",1, 0)) %>%
#   summarize(
#     lower_income = sum(lower_income == 1), 
#     lower_median_income = sum(lower_median_income == 1), 
#     median_upper_income = sum(median_upper_income == 1)  
#     )

#group by income bracket - find number of computers per category 
#lower level of computers among lower income - this bracket is underrepresented in the data comparatively speaking
computers_viz <- ggplot(states_2017_EDA, aes(x="", y=computer_count_prop, fill=income_bracket)) +
  geom_bar (width = 1, stat = "identity")
computers_viz <- computers_viz + coord_polar("y", start=0)
computers_viz 

#group by income bracket - create a boxplot
#maybe you should filter by in this 
travel_time_viz <- ggplot(states_2017_EDA, aes (x = travel_min_work_agg_prop, y = income_bracket)) + 
  geom_boxplot()
travel_time_viz

#making it interactive??? 
#girafe(ggobj1 = computers_viz) %>%
  #girafe_options(opts_hover(css = "fill:blue;"))
```

#Exploratory Data Analysis Phase 2: Understanding Variable Importance

```{r EDA}
#EDA for all states

#Means of variables for all states
means = colMeans(states_2017[,c(variable_list)])
barplot(means[order(means,decreasing=T)])

#Data Visualization for Each Variable -------------------
#med_income
summary(states_2017$med_income)
states_2017 %>%
  ggplot(aes(med_income)) +
  geom_freqpoly(binwidth = 1000) + 
  labs(x = "Median Income",
       y = "Count",
       title = "Distribtion of Median Income by Census Tract"
  )

#travel_Work
summary(states_2017$travel_work)

states_2017 %>%
  ggplot(aes(travel_work)) +
  geom_freqpoly(binwidth = 60) + # 60 seconds = 1 minute
  labs(x = "Seconds Traveled to Work",
       y = "Count",
       title = "Distribtion of Travel Time by Census Tract"
  )

#travel_min_work_agg
summary(states_2017$travel_min_work_agg)

states_2017 %>%
  ggplot(aes(travel_work)) +
  geom_freqpoly(binwidth = 60) + # 60 seconds = 1 minute
  labs(x = "Seconds Traveled to Work",
       y = "Count",
       title = "Distribtion of Travel Time by Census Tract"
  )

#computer_count
summary(states_2017$computer_count)

states_2017 %>%
  ggplot(aes(computer_count, 1)) +
  geom_point(alpha = 0.2) +
  scale_y_continuous(breaks = 0) +
  labs(y = "Computer Count by Census Tract") +
  theme_bw() +
  theme(panel.border = ggplot2::element_blank())

#female_count
summary(states_2017$female_count)

states_2017 %>%
  ggplot(aes(female_count, 1)) +
  geom_point(alpha = 0.2) +
  scale_y_continuous(breaks = 0) +
  labs(y = "Total Women by Census Tract") +
  theme_bw() +
  theme(panel.border = ggplot2::element_blank())

#total_pop
summary(states_2017$total_pop)

states_2017 %>%
  ggplot(aes(computer_count, 1)) +
  geom_point(alpha = 0.2) +
  scale_y_continuous(breaks = 0) +
  labs(y = "Total Population by Census Tract") +
  theme_bw() +
  theme(panel.border = ggplot2::element_blank())

#hs_count 
summary(states_2017$hs_count)

states_2017 %>%
  ggplot(aes(computer_count, 1)) +
  geom_point(alpha = 0.2) +
  scale_y_continuous(breaks = 0) +
  labs(y = "Total High School Degrees by Census Tract") +
  theme_bw() +
  theme(panel.border = ggplot2::element_blank())

#ged_count
summary(states_2017$ged_count)

states_2017 %>%
  ggplot(aes(computer_count, 1)) +
  geom_point(alpha = 0.2) +
  scale_y_continuous(breaks = 0) +
  labs(y = "Total GED by Census Tract") +
  theme_bw() +
  theme(panel.border = ggplot2::element_blank())

#assoc_count
summary(states_2017$assoc_count)

states_2017 %>%
  ggplot(aes(computer_count, 1)) +
  geom_point(alpha = 0.2) +
  scale_y_continuous(breaks = 0) +
  labs(y = "Total Associate's Degree by Census Tract") +
  theme_bw() +
  theme(panel.border = ggplot2::element_blank())

#bach_count
summary(states_2017$bach_count)

states_2017 %>%
  ggplot(aes(computer_count, 1)) +
  geom_point(alpha = 0.2) +
  scale_y_continuous(breaks = 0) +
  labs(y = "Total Bachelor's Degree by Census Tract") +
  theme_bw() +
  theme(panel.border = ggplot2::element_blank())

#education_alloc
summary(states_2017$education_alloc)

states_2017 %>%
  ggplot(aes(computer_count, 1)) +
  geom_point(alpha = 0.2) +
  scale_y_continuous(breaks = 0) +
  labs(y = NULL) +
  theme_bw() +
  theme(panel.border = ggplot2::element_blank())

#Data Visualizations for Relationship Between Variables ------------------
#med_income ------------------
#travel_work
states_2017 %>%
  ggplot(aes(
      x = travel_work,
      y = med_income))+
  geom_point() +
  labs(x = "Seconds Traveled to Work",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#travel_min_work_agg
states_2017 %>%
  ggplot(aes(
      x = travel_min_work_agg,
      y = med_income))+
  geom_point() +
  labs(x = "Total Seconds Traveled to Work",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#total_pop
states_2017 %>%
  ggplot(aes(
      x = total_pop,
      y = med_income))+
  geom_point() +
  labs(x = "Total Population",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#female_count
states_2017 %>%
  ggplot(aes(
      x = female_count,
      y = med_income))+
  geom_point() +
  labs(x = "Count of Females",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#computer_count
states_2017 %>%
  ggplot(aes(
      x = computer_count,
      y = med_income))+
  geom_point() +
  labs(x = "Count of Computers",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#hs_count
states_2017 %>%
  ggplot(aes(
      x = hs_count,
      y = med_income))+
  geom_point() +
  labs(x = "Count of People with Highschool Degree",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#ged_count
states_2017 %>%
  ggplot(aes(
      x = ged_count,
      y = med_income))+
  geom_point() +
  labs(x = "Count of People with GED",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#assoc_count
states_2017 %>%
  ggplot(aes(
      x = assoc_count,
      y = med_income))+
  geom_point() +
  labs(x = "Count of People with Associates Degree",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#bach_count
states_2017 %>%
  ggplot(aes(
      x = bach_count,
      y = med_income))+
  geom_point() +
  labs(x = "Count of People with Bachelors Degree",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#education_alloc
states_2017 %>%
  ggplot(aes(
      x = education_alloc,
      y = med_income))+
  geom_point() +
  labs(x = "Education Allocation",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#total_pop ------------------------
#female_count
states_2017 %>%
  ggplot(aes(
      x = female_count,
      y = total_pop))+
  geom_point() +
  labs(x = "Count of Females",
       y = "Total Population",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#computer_count
states_2017 %>%
  ggplot(aes(
      x = computer_count,
      y = total_pop))+
  geom_point() +
  labs(x = "Count of Computers",
       y = "Total Population",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#travel_min_work_agg
states_2017 %>%
  ggplot(aes(
      x = travel_min_work_agg,
      y = total_pop))+
  geom_point() +
  labs(x = "Aggregate Minutes Traveled to Work",
       y = "Total Population",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#travel_work
states_2017 %>%
  ggplot(aes(
      x = travel_work,
      y = total_pop))+
  geom_point() +
  labs(x = "Seconds Traveled to Work",
       y = "Total Population",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#hs_count
states_2017 %>%
  ggplot(aes(
      x = hs_count,
      y = total_pop))+
  geom_point() +
  labs(x = "Seconds Traveled to Work",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = ged_count,
      y = total_pop))+
  geom_point() +
  labs(x = "Seconds Traveled to Work",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = assoc_count,
      y = total_pop))+
  geom_point() +
  labs(x = "Seconds Traveled to Work",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = bach_count,
      y = total_pop))+
  geom_point() +
  labs(x = "Seconds Traveled to Work",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = education_alloc,
      y = total_pop))+
  geom_point() +
  labs(x = "Seconds Traveled to Work",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#Travel min work agg
states_2017 %>%
  ggplot(aes(
      x = female_count,
      y = travel_min_work_agg))+
  geom_point() +
  labs(x = "Seconds Traveled to Work",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = computer_count,
      y = travel_min_work_agg))+
  geom_point() +
  labs(x = "Seconds Traveled to Work",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = travel_work,
      y = travel_min_work_agg))+
  geom_point() +
  labs(x = "Seconds Traveled to Work",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = hs_count,
      y = travel_min_work_agg))+
  geom_point() +
  labs(x = "Seconds Traveled to Work",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = ged_count,
      y = travel_min_work_agg))+
  geom_point() +
  labs(x = "Seconds Traveled to Work",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = assoc_count,
      y = travel_min_work_agg))+
  geom_point() +
  labs(x = "Seconds Traveled to Work",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = bach_count,
      y = travel_min_work_agg))+
  geom_point() +
  labs(x = "Seconds Traveled to Work",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = education_alloc,
      y = travel_min_work_agg))+
  geom_point() +
  labs(x = "Seconds Traveled to Work",
       y = "Median Income",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#female count
states_2017 %>%
  ggplot(aes(
      x = computer_count,
      y = female_count))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = travel_work,
      y = female_count))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = hs_count,
      y = female_count))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = ged_count,
      y = female_count))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = assoc_count,
      y = female_count))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = bach_count,
      y = female_count))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = education_alloc,
      y = female_count))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#computer count
states_2017 %>%
  ggplot(aes(
      x = travel_work,
      y = computer_count))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = hs_count,
      y = computer_count))+
  geom_point() +
  labs(x = "Seconds Traveled to Work",
       y = "Computers in Census Tract",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = ged_count,
      y = computer_count))+
  geom_point() +
  labs(x = "Seconds Traveled to Work",
       y = "Computers in Census Tract",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = assoc_count,
      y = computer_count))+
  geom_point() +
  labs(x = "Seconds Traveled to Work",
       y = "Computers in Census Tract",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = bach_count,
      y = computer_count))+
  geom_point() +
  labs(x = "Seconds Traveled to Work",
       y = "Computers in Census Tract",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = education_alloc,
      y = computer_count))+
  geom_point() +
  labs(x = "Seconds Traveled to Work",
       y = "Computers in Census Tract",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#travel work
states_2017 %>%
  ggplot(aes(
      x = hs_count,
      y = travel_work))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = ged_count,
      y = travel_work))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = assoc_count,
      y = travel_work))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = bach_count,
      y = travel_work))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = education_alloc,
      y = travel_work))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#hs_count
states_2017 %>%
  ggplot(aes(
      x = ged_count,
      y = hs_count))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = assoc_count,
      y = hs_count))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = bach_count,
      y = hs_count))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = education_alloc,
      y = hs_count))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#ged_count
states_2017 %>%
  ggplot(aes(
      x = assoc_count,
      y = ged_count))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = bach_count,
      y = ged_count))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = bach_count,
      y = ged_count))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = education_alloc,
      y = ged_count))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#assoc_count
states_2017 %>%
  ggplot(aes(
      x = bach_count,
      y = assoc_count))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

states_2017 %>%
  ggplot(aes(
      x = education_alloc,
      y = assoc_count))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

#education alloc
states_2017 %>%
  ggplot(aes(
      x = education_alloc,
      y = bach_count))+
  geom_point() +
  labs(x = "",
       y = "",
       title = "TBD") +
  geom_smooth(method=lm, se=FALSE)

```

## splitting the data, creating folds, and creating a recipe (that will need to get updated depending on variables used)
```{r - Splitting and folding and creating a recipe}
#Set up a testing environment
#set the seed and split the data
set.seed(20211101)
states_2017_split <- initial_split(states_2017, prop = .8)
states_2017_train1 <- training(states_2017_split)
states_2017_test <- testing(states_2017_split)

#here's the dataframe we'll use for the model
states_2017_train <- states_2017_train1 %>%
  select(med_income, total_pop, ends_with("prop"), starts_with("z"))

#resampling with 10 folds
folds <- vfold_cv(data = states_2017_train, v = 10)

#crafting recipe
states_2017_rec <- recipe(med_income ~ ., data = states_2017_train) %>%
  step_naomit(all_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_filter_missing(all_predictors()) %>%
  step_log(ged_count_prop) %>%
  step_center(all_predictors(), -starts_with("z_")) 

# %>%
#   prep()
# temp <- bake(states_2017_rec, new_data = NULL)
# 
# 
# hist.data.frame(temp)

```

## linear regression model *infrastructure*
```{r - linear regression model}
#setting the linear reg model
lm_mod <- linear_reg() %>%
  set_engine(engine = "lm") %>%
  set_mode(mode = "regression")

#workflow
lm_wf <- workflow() %>%
  add_recipe(recipe = states_2017_rec) %>%
  add_model(spec = lm_mod)

#resampling using the 10-v-fold process specified above
lm_cv <- lm_wf %>%
  fit_resamples(resamples = folds)

#using RMSE to choose the best model
lm_best <- lm_cv %>%
  select_best("rmse")

#finalize the workflow using the established workflow and the best model (according to RMSE)
lm_final <- lm_wf %>%
  finalize_workflow(parameters = lm_best)

#fitting the workflow with the original data
lm_fit <- lm_final %>%
  fit(states_2017_train)

#RMSE looks consistent across folds
collect_metrics(lm_cv, summarize = FALSE) %>%
  filter(.metric == "rmse") %>%
  ggplot(mapping = aes(x = id, y = .estimate, group = .estimator, label = round(.estimate, digits = 2))) +
  geom_line() +
  geom_point() +
  geom_label(nudge_y = -500) +
  scale_y_continuous(limits = c(10000, 17500)) +
  labs(title = "Median Income: Linear Regression Model (RMSE Across 10 Folds)", 
       y = "Predicted RMSE",
       x = "Fold Number") +
  theme_minimal()

#mean RMSE across the 10 samples
collect_metrics(lm_cv) %>%
  filter(.metric == "rmse") %>%
  select(mean) %>%
  pull(mean)

lm_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 10)

```

## K nearest neighbors model
```{r - Exercise 02: knn model}
knn_mod <- nearest_neighbor(neighbors = 5) %>%
  set_engine(engine = "kknn") %>%
  set_mode(mode = "regression")

#workflow situation
knn_workflow <- workflow() %>%
  add_model(spec = knn_mod) %>%
  add_recipe(recipe = states_2017_rec)

#resampling using the 10-v-fold process specified above
knn_res <- knn_workflow %>%
  fit_resamples(resamples = folds)

#using RMSE to choose the best model
knn_best <- knn_res %>%
  select_best("rmse")

#finalize the workflow using the established workflow and the best model (according to RMSE)
knn_final <- knn_workflow %>%
  finalize_workflow(parameters = knn_best)

#fitting the workflow with the original data
knn_fit <- knn_final %>%
  fit(states_2017_train)

#applying the updated workflow to the fit across folds
knn_fit_rs <- knn_final %>%
  fit_resamples(resamples = folds)

#RMSE looks consistent across folds
collect_metrics(knn_res, summarize = FALSE) %>%
  filter(.metric == "rmse") %>%
  ggplot(mapping = aes(x = id, y = .estimate, group = .estimator, label = round(.estimate, digits = 2))) +
  geom_line() +
  geom_point() +
  geom_label(nudge_y = -500) +
  scale_y_continuous(limits = c(10000, 17500)) +
  labs(title = "Median Income: KNN Model (RMSE Across 10 Folds)", 
       y = "Predicted RMSE",
       x = "Fold Number") +
  theme_minimal()

#mean RMSE across the 10 samples
collect_metrics(knn_res) %>%
  filter(.metric == "rmse") %>%
  select(mean) %>%
  pull(mean)

```

## Random forest model
```{r - Exercise 02: random forest model}
treeNumb <- 1000
gridVal <- 10
#since random forests can be computationally heavier, we use parallel processing
cores <- parallel::detectCores()

#random forest with 1000 iterations while utilizing the parallel processing (num.threads = cores)
rf_mod1 <- rand_forest(mtry = tune(), min_n = tune(), trees = treeNumb) %>% 
  set_engine(engine = "ranger", num.threads = cores, importance = "impurity") %>%
  set_mode("regression")

#initiating the workflow for random forests
rf_workflow1 <- workflow() %>% 
  add_recipe(recipe = states_2017_rec) %>%
  add_model(spec = rf_mod1)

#tune with the given number of folds and grid-value
rf_res <- rf_workflow1 %>% 
  tune_grid(resamples = folds, grid = gridVal,
            control = control_grid(save_pred = TRUE))

#these are the best hyperparameters (mtry = , min_n = )
rf_best <- rf_res %>% 
  select_best(metric = "rmse")

#applying the best hyperparameters to our original workflow (since we used tune() as "place holders" for mtry and min_n)
rf_final <- rf_workflow1 %>%
  finalize_workflow(parameters = rf_best)

#apply the worlflow when fitting the data
rf_last_fit <- rf_final %>%
   last_fit(states_2017_split)

#fitting the workflow with the original data
rf_final_fit <- fit(rf_final, only_numb)

#applying the updated workflow to the fit
rf_fit_rs <- rf_final %>%
  fit_resamples(resamples = folds)

#mean RMSE across the 10 samples
collect_metrics(rf_fit_rs) %>%
  filter(.metric == "rmse") %>%
  select(mean) %>%
  pull(mean)

rf_last_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 10)


```

## Prediction 
```{r - prediction for linear regression}
#making a prediction
predictions_testing <- bind_cols(states_2017_test,
    predict(object = lm_fit, new_data = states_2017_test)
  )
predictions_testing

#rmse for test data
rmse(data = predictions_testing, truth = states_2017_test$med_income, estimate = .pred)


```



# Map figure 1
```{r}
#recruiting the testing data for the map figure
predictedData <- predictions_testing %>%
  select(-name, -pop, -msaname15) %>%
  group_by(state_name) %>%
  summarise(
    mean_coi = round(mean(z_coi_nat), 3),
    mean_he = round(mean(z_he_nat), 3),
    mean_ed = round(mean(z_ed_nat), 3),
    median_inc = median(med_income),
    predicted_inc = round(median(.pred), 0)) %>%
  ungroup()

#loading in the mad that looks best and removing all population-related columns
map_US <- usa_sf("lcc") %>%
  select(-starts_with("pop")) %>%
  mutate(state_name = name)

#here we're fusung the income and state data - this won't be needed later on
map_US_inc <- left_join(map_US, predictedData, by = "state_name")

#this is where we'll put all relevant data that will appear when we hover over a state
map_US_inc <- map_US_inc %>% 
  mutate(median_income = median_inc,
         hoverText = paste0("State: ", as.character(state_name), "\n", 
                            "Median Income: $", as.character(median_inc), "\n", 
                            "Predicted Median Income: $", as.character(predicted_inc), "\n",
                            "Mean COI z-score: ", as.character(mean_coi), "\n",
                            "Mean Health z-score: ", as.character(mean_he)))

#here's the figure - we can come back and make it look prettier if needed
map1 <- map_US_inc %>%
  ggplot() +
  geom_sf_interactive(size = 0.2, color = "white", aes(fill = median_income,
    data_id = hoverText, tooltip = hoverText)) +
  theme_void() +
  theme(legend.position='none')

#this initializes the interactive bit
girafe(ggobj = map1) %>%
  girafe_options(opts_hover(css = "fill:blue;"))

```

# Map figure 2
```{r}
map_US_Interactive2 <- map_US_inc %>%
  select(name, median_inc, predicted_inc, mean_coi, mean_he, mean_ed)

library(mapview)
map_US_Interactive2 %>%
  mapview(zcol = "median_inc", 
          map.types = c("CartoDB.Voyager","Esri.NatGeoWorldMap","Esri.WorldPhysical"))

```


## Census API Query Method 2 (via a url query) to pull median income and some basic mapping exploration
##I'll use this to create the unsupervised ml model so no deletions necessary yet 
```{r}
#use census API Query to get data on median household income levels in 50 states
#i'm not really using this anywhere yet 
#DON'T DELTE 
url <- str_glue("https://api.census.gov/data/2017/acs/acs5?get=NAME,B06011_001E&for=state:*")
popurl <- GET(url = url)
http_status(popurl)
popurl <- content(popurl, as = "text")
pop_matrix <- fromJSON(popurl)
#turn body of character matrix into a tibble
us_50_states <- as_tibble(pop_matrix[2:nrow(pop_matrix),],.name_repair = "minimal")
#add variable names to the tibble
names(us_50_states) <- us_50_states[1,]

us_map <- tidycensus::get_acs(
  geography = "state", 
  variables = c(total_pop = "B01003_001", 
                med_income = "B19013_001"), 
  output = "wide", 
  state = states, 
  geometry = TRUE, 
  year = 2017) 

plot(us_map["med_incomeE"])
       
#plot(ca_example_map["med_incomeE"])

#this is an interactive map
library(mapview)
mapview(us_map, zcol = "med_incomeE")
```

#Unsupervised Machine Learning Models 
```{r - unsupervised clustering}
url <- str_glue("https://api.census.gov/data/2017/acs/acs5?get=NAME,B01001_026E,B28010_001E,B15003_017E, &for=tract:*&in=state:17&in=county:031")
popurl <- GET(url = url)
http_status(popurl)
popurl <- content(popurl, as = "text")
pop_matrix <- fromJSON(popurl)
#turn body of character matrix into a tibble
states <- as_tibble(pop_matrix[1:nrow(pop_matrix),],.name_repair = "minimal")
names(states) <- states[1,]
states = states[-1,]

#take out na values
states[is.na(states)] <- 0

#only contains values 
drop <- c("NAME", "state", "county","tract")
states_numeric <- states[,!(names(states) %in% drop)]
states_numeric[1:3] <- sapply(states_numeric[1:3], as.numeric)
head(states_numeric)

#run PCA on states_numeric
states_pca <- prcomp(states_numeric, center = TRUE, scale. = TRUE)

#how much variance explained? 
summary(states_pca)

#extract principle components
states_pcs <- states_pca %>% 
  .$x %>% 
  as_tibble()

#combine pc's to names
states_pcs <- bind_cols(
  select(states, -state), 
  select(states_pcs, PC1, PC2)
)

#total within sum of squares to find ideal number of clusters
fviz_nbclust(states_numeric, FUN = kmeans, method = "wss")
#total silhouette width
fviz_nbclust(states_numeric, FUN = kmeans, method = "silhouette")
#gap statistic
fviz_nbclust(states_numeric, FUN = kmeans, method = "gap_stat")

num_kmeans <- function(k, data){
  #generate k-means clusters
  kmeans <- kmeans(states_numeric, 
  centers = k, 
  nstart = 100
  );
#create dataframe that binds together clusters and PC's
  votes_clusters <- bind_cols(
  select(states, NAME), 
  select(data, PC1, PC2), 
  cluster = kmeans$cluster
  )
}

kmeans_2 <- num_kmeans(k = 2, data = states_pcs) 
kmeans_2

fviz_cluster(km_4, data = states_numeric)

km_4 <- kmeans(states_numeric, centers = 4, nstart = 25)
km_4
clusters <- bind_cols( 
  select (states, NAME), 
  select ())
```